---
layout: post
title:  "bcache setup on ubuntu"
date:   2021-03-11 00:00:00 +1200
categories: posts
---

## Overview

backing device: a slow but large disk to store data, which you want to speed up, e.g.: /dev/sdb
bcache device: /dev/bcacheN, which created from a backing device /dev/sdb
cache device: a ssd partition formatted for bcache, /dev/nvme0n1p1

You can make/create a bcache device from a backing device without cache device present.
Just use the bcache device /dev/bcacheN as normal disk.
It will be running in `passthrough` mode, which is the same as a normal/slow disk.

Later, once you get a ssd partition ready, then you can format it as cache device,
and attach the bcache device to it, to actually speed up the backing device /dev/sdb.

## Commands

1) Get backing and bacahe device ready

switch to root and install bcache-tools:

    sudo -i
    apt install --yes bcache-tools

    make-bcache -B /dev/${SDX}
    echo /dev/${SDX} > /sys/fs/bcache/register

convert a disk to backing device and create the bcache device:

    BACKING=/dev/sdb
    make-bcache -B $BACKING

Sample output:

    UUID:                   b3bc521e-ca04-458b-a106-db874e4f8c57
    Set UUID:               f60b6bc6-a623-42f8-8643-64a5cc8f98c6
    version:                1
    block_size:             1
    data_offset:            16

Assign a var for UUID:

    DEV_UUID=b3bc521e-ca04-458b-a106-db874e4f8c57

bcache-tools now ships udev rules, and bcache devices are known to the kernel immediately.
This means, above command will generate a bcache device /dev/bcacheN:

    ll /dev/bcache*
    brw-rw---- 1 root disk 252,   0 Mar 10 05:14 /dev/bcache0
    ...
    BCACHE=/dev/bcache0

Without udev(e.g.: old bcache-tools version), you can manually register devices like this:

  echo $BACKING > /sys/fs/bcache/register

Registering the backing device makes the bcache device show up in /dev.



You can also find the bcacheN device with:

    ls -d /sys/block/bcache*/slaves/sdb
    /sys/block/bcache0/slaves/sdc
    N=0

So from here you can know, sdb is mapped to bcache0.
This way is more reliable when you have multiple bcacheN devices.

The devices show up as:

    /dev/bcache<N>

As well as (with udev):

    /dev/bcache/by-uuid/<uuid>
    /dev/bcache/by-label/<label>

Once bcache device shows up, you can now format it (e.g.: mkfs.ext4) and use it as normal disk.

    mkfs.ext4 /dev/bcache0
    mount /dev/bcache0 /mnt

2) Get cache/ssd device ready

create a cache (ssd):

    make-bcache -C /dev/nvme0n1  # a disk or a partition ?

Find existing cache sets:

    ls /sys/fs/bcache/
    9becf4a6-01b9-49a7-b135-cdd085e565a5  d4891070-50b7-40fc-ac34-3f226cf47562  register  register_quiet

If you have only 1 cache set, that's simple.
If you have multiple, you can check what other backing device are using with:

    bcache-super-show /dev/sde
    sb.magic		ok
    sb.first_sector		8 [match]
    sb.csum			9B2561BFE1986D45 [match]
    sb.version		1 [backing device]

    dev.label		bcache4
    dev.uuid		04d9956c-3acf-4447-9f90-7e17ef0efd5a
    dev.sectors_per_block	1
    dev.sectors_per_bucket	1024
    dev.data.first_sector	16
    dev.data.cache_mode	1 [writeback]
    dev.data.cache_state	2 [dirty]

    cset.uuid		d4891070-50b7-40fc-ac34-3f226cf47562

NOTE: the `dev.label bcache4` is not the /dev/bcache0 device we created, don't be misleaded by that.

Assign a var:

    CSET_UUID=d4891070-50b7-40fc-ac34-3f226cf47562

Now you can attach your bcache device to a cache:

    echo $CSET_UUID > /sys/block/bcache0/bcache/attach

The above `attach` is a write only file, you may get an error:

    echo: write error: No such file or directory

But it may actually worked, which you can verify with:

    bcache-super-show /dev/sdb | grep cset.uuid

which should have the correct cset uuid.


### juju ceph osd add new osd

If there is no bcache, then you can add a disk as osd with:

    juju run-action --wait $OSD_UNIT zap-disk devices=/dev/sdb i-really-mean-it=yes

However, if bcache in use, the devices arg becomes a bit complex.

If ceph is using bluestore, then easy:

    juju run-action --wait $OSD_UNIT zap-disk devices=/dev/bcache0 i-really-mean-it=yes

If ceph is using filestore, then:

    ll /dev/disk/by-dname/*
    lrwxrwxrwx 1 root root 13 Mar 10 05:14 /dev/disk/by-dname/bcache2 -> ../../bcache4
    lrwxrwxrwx 1 root root 13 Mar 10 05:14 /dev/disk/by-dname/bcache3 -> ../../bcache1
    lrwxrwxrwx 1 root root 13 Mar 10 05:14 /dev/disk/by-dname/bcache4 -> ../../bcache2
    lrwxrwxrwx 1 root root 13 Mar 10 05:14 /dev/disk/by-dname/bcache5 -> ../../bcache3
    lrwxrwxrwx 1 root root 13 Mar 10 05:14 /dev/disk/by-dname/bcache6 -> ../../bcache0
    lrwxrwxrwx 1 root root 13 Mar 10 05:14 /dev/disk/by-dname/bcache7 -> ../../bcache5
    lrwxrwxrwx 1 root root 13 Mar 10 05:14 /dev/disk/by-dname/bcache8 -> ../../bcache6
    ...

The `../../bcacheN` which is `/dev/bcacheN` is the actually bcache device we just created.
But we need to use the `/dev/disk/by-dname/bcacheM` thing as the devices arg in juju action:

    juju run-action --wait $OSD_UNIT zap-disk devices=/dev/disk/by-dname/bcache6 i-really-mean-it=yes

that mapping may not exists, because it's not generated correctly, which become the most difficult part.
Good news is, with the new bluestore storage backend for ceph, this is gone.

### find out ceph osd bacache device

on ceph-mon:

    ceph osd tree
    ...
    -25        18.18489         host dcs1-clp-nod17
      3   ssd   3.63698             osd.3               up  1.00000 1.00000
      9   ssd         0             osd.9               up  1.00000 1.00000
     ...

    ceph osd metadata osd.9
    ...
    "osd_data": "/var/lib/ceph/osd/ceph-9",
    ...

now on dcs1-clp-nod17:

    ll /var/lib/ceph/osd/ceph-9/
    block -> /dev/ceph-27146b9b-f8d8-4cf3-b99c-23f2a91eb266/osd-block-27146b9b-f8d8-4cf3-b99c-23f2a91eb266

With the block info, we can find this in lsblk:

    lsblk | grep 27146b9b

    NAME                                             MAJ:MIN RM   SIZE RO TYPE  MOUNTPOINT
    ...
    nvme0n1                                          259:1    0   2.9T  0 disk
    ├─nvme0n1p1                                      259:2    0 372.5G  0 part
    ...
    └─nvme0n1p2                                      259:3    0   2.6T  0 part
      │                                              253:5    0   3.7T  0 lvm
      └─bcache7                                      252:896  0   3.7T  0 disk
        └─crypt-27146b9b-f8d8-4cf3-b99c-23f2a91eb266 253:10   0   3.7T  0 crypt
          └─ceph--27146b9b--f8d8--4cf3--b99c--23f2a91eb266-osd--block--27146b9b--f8d8--4cf3--b99c--23f2a91eb266
                                                     253:11   0   3.7T  0 lvm
    sdd                                                8:48   0   3.7T  0 disk
    └─bcache7                                        252:896  0   3.7T  0 disk
      └─crypt-27146b9b-f8d8-4cf3-b99c-23f2a91eb266   253:10   0   3.7T  0 crypt
        └─ceph--27146b9b--f8d8--4cf3--b99c--23f2a91eb266-osd--block--27146b9b--f8d8--4cf3--b99c--23f2a91eb266
     ...

From here, we can see ceph osd.9 is using:

- sdd as backing device
- /dev/bcache7 as bcache device
- nvme0n1p2 as cache device

However, /dev/bcache7 doesn't has a by-dname link:

    root@dcs1-clp-nod17:/var/lib/ceph/osd/ceph-9# ll /dev/disk/by-dname/*
    lrwxrwxrwx 1 root root 13 Mar 10 05:14 /dev/disk/by-dname/bcache2 -> ../../bcache4
    lrwxrwxrwx 1 root root 13 Mar 10 05:14 /dev/disk/by-dname/bcache3 -> ../../bcache1
    lrwxrwxrwx 1 root root 13 Mar 10 05:14 /dev/disk/by-dname/bcache4 -> ../../bcache2
    lrwxrwxrwx 1 root root 13 Mar 10 05:14 /dev/disk/by-dname/bcache5 -> ../../bcache3
    lrwxrwxrwx 1 root root 13 Mar 10 05:14 /dev/disk/by-dname/bcache6 -> ../../bcache0
    lrwxrwxrwx 1 root root 13 Mar 10 05:14 /dev/disk/by-dname/bcache7 -> ../../bcache5
    lrwxrwxrwx 1 root root 13 Mar 10 05:14 /dev/disk/by-dname/bcache8 -> ../../bcache6


### start ceph osd recovery/rebalance in fast mode

    ceph osd tree

    OSD_ID=9
    OSD_HOST=dcs1-clp-nod17
    OSD_WEIGHT=3.63698

    ceph tell osd.${OSD_ID} injectargs --osd-max-backfills=3 --osd-recovery-max-active=9 --osd-recovery-op-priority=1
    ceph osd crush set osd.${OSD_ID} ${OSD_WEIGHT} root=${OSD_HOST}

## references

[0]: https://www.kernel.org/doc/Documentation/bcache.txt
[1]: https://www.kernel.org/doc/html/latest/admin-guide/bcache.html